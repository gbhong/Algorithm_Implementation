{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Piece Tokenizer\n",
    "\n",
    "<br> https://lovit.github.io/nlp/2018/04/02/wpm/\n",
    "<br> https://wikidocs.net/22592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for dataframe\n",
    "import numpy as np # for numpy data structure\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression # baseline model for classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '3_class_naver_news_200819_test.csv',\n",
       " '3_class_naver_news_200819_train.csv']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check directory\n",
    "os.listdir('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20170108</td>\n",
       "      <td>IT_science</td>\n",
       "      <td>데일리안</td>\n",
       "      <td>‘효자폰’ 갤럭시S7 ‘불효폰’ G5…분위기 극과극</td>\n",
       "      <td>삼성전자 ‘갤럭시S7’ 왼쪽 LG전자 ‘G5’ 오른쪽 모델 이미지 ⓒ각 사 삼성전자...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20170111</td>\n",
       "      <td>IT_science</td>\n",
       "      <td>디지털타임스</td>\n",
       "      <td>카카오 O2O 플랫폼 사업 윤곽도 못잡아</td>\n",
       "      <td>11월초 공개 O2O 위드 카카오 두달 넘게 모습 드러내지 않아 후발주자로 출시까지...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20170103</td>\n",
       "      <td>economy</td>\n",
       "      <td>조세일보</td>\n",
       "      <td>동원 고위도高緯度 북대서양 참다랑어 어획 성공</td>\n",
       "      <td>동원산업 대표이사 이명우 은 고위도 高緯度 북대서양 참다랑어를 어획하는데 성공했다고...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1           2       3                             4  \\\n",
       "0  0  20170108  IT_science    데일리안  ‘효자폰’ 갤럭시S7 ‘불효폰’ G5…분위기 극과극   \n",
       "1  1  20170111  IT_science  디지털타임스        카카오 O2O 플랫폼 사업 윤곽도 못잡아   \n",
       "2  2  20170103     economy    조세일보     동원 고위도高緯度 북대서양 참다랑어 어획 성공   \n",
       "\n",
       "                                                   5  \\\n",
       "0  삼성전자 ‘갤럭시S7’ 왼쪽 LG전자 ‘G5’ 오른쪽 모델 이미지 ⓒ각 사 삼성전자...   \n",
       "1  11월초 공개 O2O 위드 카카오 두달 넘게 모습 드러내지 않아 후발주자로 출시까지...   \n",
       "2  동원산업 대표이사 이명우 은 고위도 高緯度 북대서양 참다랑어를 어획하는데 성공했다고...   \n",
       "\n",
       "                                                   6  \n",
       "0  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
       "1  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
       "2  https://news.naver.com/main/read.nhn?mode=LSD&...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/3_class_naver_news_200819_train.csv', header=None)\n",
    "test = pd.read_csv('./data/3_class_naver_news_200819_test.csv', header=None)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print()\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['index', 'date', 'category', 'media', 'title', 'contents', 'link']\n",
    "train.columns = colnames\n",
    "test.columns = colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Wordpiece Tokenization example\n",
    "- Word Piece를 활용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Iteration 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('e', 's')\n",
      "dictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('es', 't')\n",
      "dictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('est', '</w>')\n",
      "dictionary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('l', 'o')\n",
      "dictionary: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('lo', 'w')\n",
      "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('n', 'e')\n",
      "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 7"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('ne', 'w')\n",
      "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('new', 'est</w>')\n",
      "dictionary: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('low', '</w>')\n",
      "dictionary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('w', 'i')\n",
      "dictionary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int) # int를 value로 하는 dictionary 자료 구조 생성\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split() # [l, o, w]\n",
    "        \n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq # key로 word pair tuple을 받아서 갯수 카운트\n",
    "            \n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    '''\n",
    "    ARGS:\n",
    "        pair -> subword pairs which were exposed most frequently\n",
    "        v_in -> vocab\n",
    "    '''\n",
    "    v_out = {}\n",
    "    \n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word) # ['n','e','w','es','t']\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "# 예시 vocab 사전\n",
    "vocab = {\n",
    "        'l o w </w>' : 5,\n",
    "         'l o w e r </w>' : 2,\n",
    "         'n e w e s t </w>': 6,\n",
    "         'w i d e s t </w>': 3\n",
    "         }\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "#     display(Markdown(\"### Iteration {}\".format(i + 1)))\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key = pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "\n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "\n",
    "    print(\"new merge: {}\".format(best))\n",
    "    print(\"dictionary: {}\".format(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('e', 's'): 0,\n",
       " ('es', 't'): 1,\n",
       " ('est', '</w>'): 2,\n",
       " ('l', 'o'): 3,\n",
       " ('lo', 'w'): 4,\n",
       " ('n', 'e'): 5,\n",
       " ('ne', 'w'): 6,\n",
       " ('new', 'est</w>'): 7,\n",
       " ('low', '</w>'): 8,\n",
       " ('w', 'i'): 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Mecab-based word extraction to build pre-trained dict\n",
    "<br> 10,000개의 train data를 활용해서 n-gram dict를 만듭니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0th document...\n",
      "processing 1000th document...\n",
      "processing 2000th document...\n",
      "processing 3000th document...\n",
      "processing 4000th document...\n",
      "processing 5000th document...\n",
      "processing 6000th document...\n",
      "processing 7000th document...\n",
      "processing 8000th document...\n",
      "processing 9000th document...\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from collections import defaultdict\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "def to_ngrams(words, n):\n",
    "    ngrams = []\n",
    "    for idx in range(len(words) - n + 1):\n",
    "        ngrams.append(tuple(words[idx:idx+n]))\n",
    "    return ngrams\n",
    "\n",
    "def ngram_extractor(docs:list, min_count=5, n_range=(1,2)):\n",
    "    \n",
    "    '''\n",
    "        input\n",
    "        docs -> list whose elements are strings\n",
    "        min_count -> minimum threshold for dictionary, default 5.\n",
    "        n_range -> range of n-gram, default is for unigram and bigram.\n",
    "        \n",
    "        output\n",
    "        ngram_counter -> counting dictionary for n-gram\n",
    "    '''\n",
    "\n",
    "    ngram_counter = defaultdict(int) # count를 위한 dictionary를 만들기 위해 defaultdict 사용\n",
    "    begin, end = n_range\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(f'processing {idx}th document...')\n",
    "        \n",
    "        unigrams = mecab.pos(docs[idx], join=True)\n",
    "        for n in range(begin, end + 1):\n",
    "            for ngram in to_ngrams(unigrams, n):\n",
    "                ngram_counter[ngram] += 1\n",
    "\n",
    "    # set min_count for the size of dict\n",
    "    ngram_counter = {\n",
    "        ngram:count for ngram, count in ngram_counter.items()\n",
    "        if count >= min_count\n",
    "    }\n",
    "    \n",
    "    return ngram_counter\n",
    "\n",
    "# 뉴스의 본문 token을 활용한 tokenizer 만들기\n",
    "ngram_dic = ngram_extractor(train['contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('./SF',), 113157),\n",
       " (('을/JKO',), 87622),\n",
       " (('다/EF',), 79442),\n",
       " (('다/EF', './SF'), 76198),\n",
       " (('에/JKB',), 60266),\n",
       " (('의/JKG',), 59262),\n",
       " (('를/JKO',), 57213),\n",
       " (('이/JKS',), 54208),\n",
       " (('는/JX',), 44613),\n",
       " (('하/XSV',), 43819),\n",
       " (('는/ETM',), 42304),\n",
       " (('고/EC',), 42134),\n",
       " (('은/JX',), 40539),\n",
       " (('가/JKS',), 34990),\n",
       " (('으로/JKB',), 32197),\n",
       " (('이/VCP',), 28398),\n",
       " (('에서/JKB',), 27844),\n",
       " (('했/XSV+EP',), 27148),\n",
       " (('로/JKB',), 22985),\n",
       " (('도/JX',), 22290)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check built vocabs\n",
    "sorted(ngram_dic.items(), key=lambda x:-x[1])[:20]\n",
    "# 조사, 동사가 대부분. 이들은 정보성이 부족."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('기술/NNG',) 5112\n",
      "('과학/NNG', '기술/NNG') 272\n",
      "('정보/NNG', '기술/NNG') 221\n",
      "('통신/NNG', '기술/NNG') 219\n",
      "('는/ETM', '기술/NNG') 174\n",
      "('신/XPN', '기술/NNG') 156\n",
      "('의/JKG', '기술/NNG') 131\n",
      "('AI/SL', '기술/NNG') 107\n",
      "('첨단/NNG', '기술/NNG') 90\n",
      "('한/XSA+ETM', '기술/NNG') 87\n",
      "('핵심/NNG', '기술/NNG') 86\n",
      "('혁신/NNG', '기술/NNG') 71\n",
      "('주행/NNG', '기술/NNG') 63\n",
      "('인공지능/NNP', '기술/NNG') 62\n",
      "('원천/NNG', '기술/NNG') 58\n",
      "('인식/NNG', '기술/NNG') 54\n",
      "('관련/NNG', '기술/NNG') 53\n",
      "('산업/NNG', '기술/NNG') 52\n",
      "('미래/NNG', '기술/NNG') 51\n",
      "('닷/NR', '기술/NNG') 50\n",
      "('는/JX', '기술/NNG') 49\n",
      "('새로운/VA+ETM', '기술/NNG') 46\n",
      "('IT/SL', '기술/NNG') 46\n",
      "('과/JC', '기술/NNG') 44\n",
      "('./SF', '기술/NNG') 43\n",
      "('이/MM', '기술/NNG') 41\n",
      "('한/XSV+ETM', '기술/NNG') 40\n",
      "('인/VCP+ETM', '기술/NNG') 39\n",
      "('러닝/NNG', '기술/NNG') 38\n",
      "('최고/NNG', '기술/NNG') 37\n",
      "('소비자/NNG', '기술/NNG') 36\n",
      "('IoT/SL', '기술/NNG') 34\n",
      "('고/EC', '기술/NNG') 32\n",
      "('은/JX', '기술/NNG') 32\n",
      "('최신/NNG', '기술/NNG') 31\n",
      "('등/NNB', '기술/NNG') 31\n",
      "('된/XSV+ETM', '기술/NNG') 29\n",
      "('셀/VV+ETM', '기술/NNG') 29\n",
      "('VR/SL', '기술/NNG') 29\n",
      "('융합/NNG', '기술/NNG') 28\n",
      "('ICT/SL', '기술/NNG') 28\n",
      "('은/ETM', '기술/NNG') 25\n",
      "('에/JKB', '기술/NNG') 25\n",
      "('’/SY', '기술/NNG') 25\n",
      "('위한/VV+ETM', '기술/NNG') 24\n",
      "('블록체인/NNG', '기술/NNG') 24\n",
      "('AR/SL', '기술/NNG') 23\n",
      "('처리/NNG', '기술/NNG') 22\n",
      "('음향/NNG', '기술/NNG') 22\n",
      "('화/XSN', '기술/NNG') 21\n",
      "('국가/NNG', '기술/NNG') 21\n",
      "('이/JKS', '기술/NNG') 20\n",
      "('와/JC', '기술/NNG') 20\n",
      "('보안/NNG', '기술/NNG') 20\n",
      "('·/SC', '기술/NNG') 20\n",
      "('적/XSN', '기술/NNG') 19\n",
      "('기반/NNG', '기술/NNG') 18\n",
      "('에서/JKB', '기술/NNG') 18\n",
      "('해/XSV+EC', '기술/NNG') 18\n",
      "('제조/NNG', '기술/NNG') 17\n",
      "('제품/NNG', '기술/NNG') 17\n",
      "('G/SL', '기술/NNG') 17\n",
      "('및/MAJ', '기술/NNG') 17\n",
      "('안전/NNG', '기술/NNG') 16\n",
      "('형/XSN', '기술/NNG') 15\n",
      "('공공/NNG', '기술/NNG') 15\n",
      "('반도체/NNG', '기술/NNG') 15\n",
      "('차/NNG', '기술/NNG') 15\n",
      "('“/SSO', '기술/NNG') 14\n",
      "('표준/NNG', '기술/NNG') 14\n",
      "('중소기업/NNG', '기술/NNG') 13\n",
      "('인터넷/NNG', '기술/NNG') 13\n",
      "('가/JKS', '기술/NNG') 12\n",
      "('통해/VV+EC', '기술/NNG') 12\n",
      "('도/JX', '기술/NNG') 12\n",
      "('자체/NNG', '기술/NNG') 12\n",
      "('‘/SY', '기술/NNG') 12\n",
      "('으로/JKB', '기술/NNG') 12\n",
      "('디스플레이/NNG', '기술/NNG') 12\n",
      "('위해/VV+EC', '기술/NNG') 12\n",
      "('드론/NNP', '기술/NNG') 12\n",
      "('자동차/NNG', '기술/NNG') 12\n",
      "('로봇/NNG', '기술/NNG') 11\n",
      "('나노/NNP', '기술/NNG') 11\n",
      "('인증/NNG', '기술/NNG') 11\n",
      "('로/JKB', '기술/NNG') 10\n",
      "('OLED/SL', '기술/NNG') 10\n",
      "('핀테크/NNG', '기술/NNG') 10\n",
      "('디지털/NNG', '기술/NNG') 10\n",
      "('시스템/NNG', '기술/NNG') 10\n",
      "('주요/NNG', '기술/NNG') 10\n",
      "('번역/NNG', '기술/NNG') 10\n",
      "('선행/NNG', '기술/NNG') 10\n",
      "('데이터/NNG', '기술/NNG') 10\n",
      "('합성/NNG', '기술/NNG') 10\n",
      "('오디오/NNG', '기술/NNG') 9\n",
      "('제어/NNG', '기술/NNG') 9\n",
      "('유망/NNG', '기술/NNG') 9\n",
      "('생산/NNG', '기술/NNG') 9\n",
      "('무전/NNG', '기술/NNG') 9\n",
      "('프린팅/NNG', '기술/NNG') 9\n",
      "('D/SL', '기술/NNG') 8\n",
      "('해당/NNG', '기술/NNG') 8\n",
      "('광학/NNG', '기술/NNG') 8\n",
      "('센서/NNG', '기술/NNG') 8\n",
      "('차세대/NNG', '기술/NNG') 8\n",
      "('오직/MAG', '기술/NNG') 8\n",
      "('부/NNG', '기술/NNG') 8\n",
      "('우수/NNG', '기술/NNG') 8\n",
      "('종합/NNG', '기술/NNG') 8\n",
      "('공정/NNG', '기술/NNG') 8\n",
      "('암호/NNG', '기술/NNG') 8\n",
      "('공학/NNG', '기술/NNG') 8\n",
      "('나노/NNG', '기술/NNG') 8\n",
      "('글로벌/NNG', '기술/NNG') 8\n",
      "('을/JKO', '기술/NNG') 7\n",
      "('관리/NNG', '기술/NNG') 7\n",
      "('이런/MM', '기술/NNG') 7\n",
      "('금융/NNG', '기술/NNG') 7\n",
      "('카/NNG', '기술/NNG') 7\n",
      "('복합/NNG', '기술/NNG') 7\n",
      "('Cell/SL', '기술/NNG') 7\n",
      "('선진/NNG', '기술/NNG') 7\n",
      "('특허/NNG', '기술/NNG') 7\n",
      "('충전/NNG', '기술/NNG') 7\n",
      "('국내/NNG', '기술/NNG') 7\n",
      "('CCTV/SL', '기술/NNG') 7\n",
      "('요소/NNG', '기술/NNG') 7\n",
      "('고급/NNG', '기술/NNG') 7\n",
      "('추천/NNG', '기술/NNG') 7\n",
      "('최첨단/NNG', '기술/NNG') 7\n",
      "('소프트웨어/NNG', '기술/NNG') 7\n",
      "('음성/NNG', '기술/NNG') 7\n",
      "('평가/NNG', '기술/NNG') 7\n",
      "('분야/NNG', '기술/NNG') 6\n",
      "('뛰어난/VA+ETM', '기술/NNG') 6\n",
      "('가상현실/NNP', '기술/NNG') 6\n",
      "('증강현실/NNP', '기술/NNG') 6\n",
      "('모스/NNG', '기술/NNG') 6\n",
      "('봇/NNG', '기술/NNG') 6\n",
      "('선도/NNG', '기술/NNG') 6\n",
      "('클라우드/NNP', '기술/NNG') 6\n",
      "('또/MAG', '기술/NNG') 6\n",
      "('통한/VV+ETM', '기술/NNG') 6\n",
      "('대한/VV+ETM', '기술/NNG') 6\n",
      "('응용/NNG', '기술/NNG') 6\n",
      "('분석/NNG', '기술/NNG') 6\n",
      "('비/XPN', '기술/NNG') 6\n",
      "('지만/EC', '기술/NNG') 5\n",
      "('에게/JKB', '기술/NNG') 5\n",
      "('연계/NNG', '기술/NNG') 5\n",
      "('새/MM', '기술/NNG') 5\n",
      "('올해/NNG', '기술/NNG') 5\n",
      "('우리/NP', '기술/NNG') 5\n",
      "('네트워크/NNG', '기술/NNG') 5\n",
      "('라며/EC', '기술/NNG') 5\n",
      "('학습/NNG', '기술/NNG') 5\n",
      "('카메라/NNG', '기술/NNG') 5\n",
      "('분산/NNG', '기술/NNG') 5\n",
      "('도록/EC', '기술/NNG') 5\n",
      "('전송/NNG', '기술/NNG') 5\n",
      "('위/NNG', '기술/NNG') 5\n",
      "('가지/NNBC', '기술/NNG') 5\n",
      "('원장/NNG', '기술/NNG') 5\n",
      "('발전/NNG', '기술/NNG') 5\n",
      "('하지만/MAJ', '기술/NNG') 5\n"
     ]
    }
   ],
   "source": [
    "# 주어진 vocab dict sort해서 보기\n",
    "for ngram, count in sorted(ngram_dic.items(), key=lambda x:-x[1]):\n",
    "    if ngram[-1] == '기술/NNG':\n",
    "        print(ngram, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. TF-IDF Vectorizer code implementation\n",
    "\n",
    "- 주어진 문서들의 토큰을 column으로, 문서를 row로 하는 TFIDF-based sparse matrix 직접 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(token for doc in docs for token in mecab.morphs(doc)))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs) # N= 4  \n",
    "\n",
    "def tf(sent:str, token:str):\n",
    "    return mecab.morphs(sent).count(token)\n",
    "\n",
    "# token -> vocab dictionary 단어들 중 하나\n",
    "def idf(token:str):\n",
    "    global N, docs\n",
    "    \n",
    "    cnt = 0\n",
    "    for doc in docs:\n",
    "        if token in mecab.morphs(doc):\n",
    "            cnt += 1\n",
    "            \n",
    "    return np.log(N / (cnt + 1))\n",
    "\n",
    "def tfidf(sent:str, token:str):\n",
    "    return tf(sent, token) * idf(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>고</th>\n",
       "      <th>과일</th>\n",
       "      <th>길</th>\n",
       "      <th>노란</th>\n",
       "      <th>는</th>\n",
       "      <th>먹</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶</th>\n",
       "      <th>아요</th>\n",
       "      <th>은</th>\n",
       "      <th>이</th>\n",
       "      <th>저</th>\n",
       "      <th>좋</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     고        과일         길        노란         는         먹       바나나        사과  \\\n",
       "0  0.0  0.000000  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147   \n",
       "1  0.0  0.000000  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000   \n",
       "2  0.0  0.000000  0.693147  0.693147  0.000000  0.000000  0.575364  0.000000   \n",
       "3  0.0  0.693147  0.000000  0.000000  0.693147  0.000000  0.000000  0.000000   \n",
       "\n",
       "          싶        아요         은         이         저         좋  \n",
       "0  0.287682  0.000000  0.287682  0.000000  0.000000  0.000000  \n",
       "1  0.287682  0.000000  0.287682  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.693147  0.000000  0.693147  0.693147  0.693147  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [[0.0, 0.0, 0.0, 0.0, 0.287, 0.0, ... 0.0], [0.0. 0.0 0.0], [], []]\n",
    "\n",
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    for j in range(len(vocab)):\n",
    "        result[-1].append(tfidf(docs[i], vocab[j]))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Use PMI score for extracting/filtering N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_score(ngram_counter, delta=30):\n",
    "    ngrams_ = {}\n",
    "    for ngram, count in ngram_counter.items():\n",
    "        \n",
    "        if len(ngram) == 1:\n",
    "            continue\n",
    "            \n",
    "        first = ngram_counter[ngram[:-1]]\n",
    "        second = ngram_counter[ngram[1:]]\n",
    "        score = (count - delta) / (first * second) # consider the role of delta\n",
    "        \n",
    "        if score > 0: # pmi score가 양수인 경우에만 추가\n",
    "            ngrams_[ngram] = (count, score)\n",
    "            \n",
    "    return ngrams_\n",
    "\n",
    "# 위에서 받은 ngram_dic을 인자로 전달해서, pmi score 확인하기\n",
    "ngram_scores = get_ngram_score(ngram_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build N-gram tokenizer\n",
    "<br> tokenize documents using pre-trained Ngram tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Ngram_tokenizer_MeCab():\n",
    "    '''\n",
    "        input\n",
    "        vocab_dic -> pretrained n-gram vocab dictionary\n",
    "        n_range -> range for n in n-gram\n",
    "    '''\n",
    "    def __init__(self, vocab_dic, n_range=(1,2)):\n",
    "        self.vocab_dic = vocab_dic\n",
    "        self.begin, self.end = n_range\n",
    "        \n",
    "    def __call__(self, sent):\n",
    "        return self.tokenize(sent)\n",
    "\n",
    "    def tokenize(self, sent):\n",
    "        '''\n",
    "            input\n",
    "            sent -> a string or document, which is to be tokenized\n",
    "            \n",
    "            output\n",
    "            ngrams -> tokenized result for given string or document\n",
    "        '''\n",
    "        if not sent:\n",
    "            return []\n",
    "        \n",
    "        unigrams = mecab.pos(sent, join=True)\n",
    "        \n",
    "        ngrams = []\n",
    "        for n in range(self.begin, self.end + 1):\n",
    "            for ngram in self._to_ngrams(unigrams, n):\n",
    "                ngrams.append('-'.join(ngram)) # to make it clear\n",
    "        return ngrams\n",
    "\n",
    "    def _to_ngrams(self, tokens, n): # returns n-gram for given window size n\n",
    "        ngrams = []\n",
    "        for idx in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[idx:idx+n])\n",
    "            if ngram in self.vocab_dic:\n",
    "                ngrams.append(ngram)\n",
    "                \n",
    "        return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Ngram_tokenizer_MeCab(vocab_dic = ngram_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['학교/NNG',\n",
       " '에서/JKB',\n",
       " '수업/NNG',\n",
       " '을/JKO',\n",
       " '받/VV',\n",
       " '는다/EF',\n",
       " './SF',\n",
       " '학교/NNG-에서/JKB',\n",
       " '수업/NNG-을/JKO',\n",
       " '을/JKO-받/VV',\n",
       " '받/VV-는다/EF',\n",
       " '는다/EF-./SF']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test case for unigram and bigram\n",
    "tokenizer.tokenize('학교에서 수업을 받는다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add new column for title tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>media</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20170108</td>\n",
       "      <td>IT_science</td>\n",
       "      <td>데일리안</td>\n",
       "      <td>‘효자폰’ 갤럭시S7 ‘불효폰’ G5…분위기 극과극</td>\n",
       "      <td>삼성전자 ‘갤럭시S7’ 왼쪽 LG전자 ‘G5’ 오른쪽 모델 이미지 ⓒ각 사 삼성전자...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20170111</td>\n",
       "      <td>IT_science</td>\n",
       "      <td>디지털타임스</td>\n",
       "      <td>카카오 O2O 플랫폼 사업 윤곽도 못잡아</td>\n",
       "      <td>11월초 공개 O2O 위드 카카오 두달 넘게 모습 드러내지 않아 후발주자로 출시까지...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20170103</td>\n",
       "      <td>economy</td>\n",
       "      <td>조세일보</td>\n",
       "      <td>동원 고위도高緯度 북대서양 참다랑어 어획 성공</td>\n",
       "      <td>동원산업 대표이사 이명우 은 고위도 高緯度 북대서양 참다랑어를 어획하는데 성공했다고...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      date    category   media                         title  \\\n",
       "0      0  20170108  IT_science    데일리안  ‘효자폰’ 갤럭시S7 ‘불효폰’ G5…분위기 극과극   \n",
       "1      1  20170111  IT_science  디지털타임스        카카오 O2O 플랫폼 사업 윤곽도 못잡아   \n",
       "2      2  20170103     economy    조세일보     동원 고위도高緯度 북대서양 참다랑어 어획 성공   \n",
       "\n",
       "                                            contents  \\\n",
       "0  삼성전자 ‘갤럭시S7’ 왼쪽 LG전자 ‘G5’ 오른쪽 모델 이미지 ⓒ각 사 삼성전자...   \n",
       "1  11월초 공개 O2O 위드 카카오 두달 넘게 모습 드러내지 않아 후발주자로 출시까지...   \n",
       "2  동원산업 대표이사 이명우 은 고위도 高緯度 북대서양 참다랑어를 어획하는데 성공했다고...   \n",
       "\n",
       "                                                link  \n",
       "0  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
       "1  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
       "2  https://news.naver.com/main/read.nhn?mode=LSD&...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>media</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>link</th>\n",
       "      <th>title_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20170108</td>\n",
       "      <td>IT_science</td>\n",
       "      <td>데일리안</td>\n",
       "      <td>‘효자폰’ 갤럭시S7 ‘불효폰’ G5…분위기 극과극</td>\n",
       "      <td>삼성전자 ‘갤럭시S7’ 왼쪽 LG전자 ‘G5’ 오른쪽 모델 이미지 ⓒ각 사 삼성전자...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>[‘/SY, 효자/NNG, 폰/NNG, ’/SY, 갤럭시/NNP, S/SL, 7/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20170111</td>\n",
       "      <td>IT_science</td>\n",
       "      <td>디지털타임스</td>\n",
       "      <td>카카오 O2O 플랫폼 사업 윤곽도 못잡아</td>\n",
       "      <td>11월초 공개 O2O 위드 카카오 두달 넘게 모습 드러내지 않아 후발주자로 출시까지...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>[카카오/NNP, O/SL, 2/SN, O/SL, 플랫/NNG, 폼/NNG, 사업/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20170103</td>\n",
       "      <td>economy</td>\n",
       "      <td>조세일보</td>\n",
       "      <td>동원 고위도高緯度 북대서양 참다랑어 어획 성공</td>\n",
       "      <td>동원산업 대표이사 이명우 은 고위도 高緯度 북대서양 참다랑어를 어획하는데 성공했다고...</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>[동원/NNG, 고위/NNG, 도/JX, 高/XPN, 북대서양/NNG, 참다랑어/N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      date    category   media                         title  \\\n",
       "0      0  20170108  IT_science    데일리안  ‘효자폰’ 갤럭시S7 ‘불효폰’ G5…분위기 극과극   \n",
       "1      1  20170111  IT_science  디지털타임스        카카오 O2O 플랫폼 사업 윤곽도 못잡아   \n",
       "2      2  20170103     economy    조세일보     동원 고위도高緯度 북대서양 참다랑어 어획 성공   \n",
       "\n",
       "                                            contents  \\\n",
       "0  삼성전자 ‘갤럭시S7’ 왼쪽 LG전자 ‘G5’ 오른쪽 모델 이미지 ⓒ각 사 삼성전자...   \n",
       "1  11월초 공개 O2O 위드 카카오 두달 넘게 모습 드러내지 않아 후발주자로 출시까지...   \n",
       "2  동원산업 대표이사 이명우 은 고위도 高緯度 북대서양 참다랑어를 어획하는데 성공했다고...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://news.naver.com/main/read.nhn?mode=LSD&...   \n",
       "1  https://news.naver.com/main/read.nhn?mode=LSD&...   \n",
       "2  https://news.naver.com/main/read.nhn?mode=LSD&...   \n",
       "\n",
       "                                         title_token  \n",
       "0  [‘/SY, 효자/NNG, 폰/NNG, ’/SY, 갤럭시/NNP, S/SL, 7/S...  \n",
       "1  [카카오/NNP, O/SL, 2/SN, O/SL, 플랫/NNG, 폼/NNG, 사업/...  \n",
       "2  [동원/NNG, 고위/NNG, 도/JX, 高/XPN, 북대서양/NNG, 참다랑어/N...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title_token'] = train['title'].apply(lambda sent: tokenizer.tokenize(sent))\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‘/SY', '효자/NNG', '폰/NNG', '’/SY', '갤럭시/NNP', 'S/SL', '7/SN', '‘/SY', '폰/NNG', '’/SY', 'G/SL', '5/SN', '…/SE', '분위기/NNG', '극/NNG', '과/JC', '극/NNG', '폰/NNG-’/SY', '갤럭시/NNP-S/SL', 'S/SL-7/SN', '폰/NNG-’/SY', 'G/SL-5/SN']\n"
     ]
    }
   ],
   "source": [
    "print(train['title_token'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. form DTM using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer = tokenizer,\n",
    "    lowercase = False,\n",
    ")\n",
    "X_train = vectorizer.fit_transform(train['title'])\n",
    "X_train.shape # (10000, 28962)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer = tokenizer,\n",
    "    lowercase = False,\n",
    ")\n",
    "X_train = vectorizer.fit_transform(train['title'])\n",
    "X_train.shape # (10000, 28962)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Piece tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 설치 필요\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'spm_input.txt'\n",
    "\n",
    "corpus = list(train['contents'])\n",
    "with open(input_file, 'w', encoding='utf-8') as f:\n",
    "    for sent in corpus:\n",
    "        f.write('{}\\n'.format(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = '--input={} --model_prefix={} --vocab_size={}'\n",
    "\n",
    "vocab_size = 10000\n",
    "prefix = '3_class_naver_news'\n",
    "cmd = templates.format(input_file, prefix, vocab_size)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '넑']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.EncodeAsPieces('넑')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.EncodeAsIds('넑')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabs = 10000\n"
     ]
    }
   ],
   "source": [
    "with open('{}.vocab'.format(prefix), encoding='utf-8') as f:\n",
    "    vocabs = [doc.strip() for doc in f]\n",
    "\n",
    "print('num of vocabs = {}'.format(len(vocabs))) # 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer = tokenizer,\n",
    "    lowercase = False,\n",
    ")\n",
    "X_train = vectorizer.fit_transform(train['title'])\n",
    "X_train.shape # (10000, 28962)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression and its coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "y_train = train['category']\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_idxs_coef = list(enumerate(classifier.coef_[0]))\n",
    "IT_idxs = sorted(IT_idxs_coef, key=lambda x:-x[1])[:50]\n",
    "\n",
    "eco_idxs_coef = list(enumerate(classifier.coef_[1]))\n",
    "eco_idxs = sorted(eco_idxs_coef, key=lambda x:-x[1])[:50]\n",
    "\n",
    "poli_idxs_coef = list(enumerate(classifier.coef_[2]))\n",
    "poli_idxs = sorted(poli_idxs_coef, key=lambda x:-x[1])[:50]\n",
    "\n",
    "vocab2idx = vectorizer.vocabulary_\n",
    "idx2vocab = list(sorted(vocab2idx, key=lambda x:vocab2idx[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IT_idxs = [(idx2vocab[idx], coef) for (idx, coef) in IT_idxs]\n",
    "IT_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eco_idxs = [(idx2vocab[idx], coef) for (idx, coef) in eco_idxs]\n",
    "eco_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poli_idxs = [(idx2vocab[idx], coef) for (idx, coef) in poli_idxs]\n",
    "poli_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test['title'])\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test['category']\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_true, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix and see which category is well-classified\n",
    "# what do we need to do more to enhance its performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
